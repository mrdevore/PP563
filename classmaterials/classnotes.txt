28 January 2025
    git pull = pull changes from my GitHub repository to my local one
    git pull upstream master = pull any changes from original owner's repository into my forked repository

    If there are different versions of a file between my local and GitHub repositories, I will need to
    manually open the file and change it in my text editor by deleting the parts I don't want, then pushing
    it to GitHub

    escape vims: :wq!


30 January 2025
    What is multiple sequence alignment (MSA)?
        - alignment introduces gaps to align sequences that are similar but not identical (we don't know the
        exact position in ancestors)
            - assumption: each line/column shares a common ancestor --> maybe there was a deletion or insertion
            since the common ancestor, that would leave a gap
            - we align sequences without knowing the processes that got us there, MSA helps us get there
                - purely computational, MSA aligns without knowledge of biological processes
                    - super computationally intensive!!
                - would be ideal if computer could spit out the right answer each time, but often the computer
                cannot pick the best one --> biologists must have input/opinion and make decisions at every step
        - ACAT > AGAT example:
            - would think it was a substitution
            - we have to include/decide on the cost of substitutions and the cost of indels (aka gaps) --> will direct
            computer's decisions between trees
                - cost = probability of it happening
                    - often we don't know, but we have to choose anyway
        - ACATTA > TACA based on known events (see class page)
            solution: -ACATTA
                    T--AC-A
            intutive version: -ACATTA
                            TACA---
        - alignment represents a series of events, going forward we will assume we do not have the correct alignment
            - we guide the algorithm by picking penalties for events (indels, subs)

    MSA algorithm
        - Cost of evolutionary events (going forward, check costs in software I use)
            - Ex: S=AACT S'=CTGG, indel cost=1 sub cost=3
                solution: AACT--
                        --CTGG
                        total cost = 4
            - Ex: S=AACT S'=CTGG, indel cost=4 sub cost=1
                solution: AACT
                        CTGG
                        total cost = 4
            - alignment changes even with same input based on costs assigned
                - knowledge of organism/biology can inform cost assignments, but we can never actually know
                    - CSL suggests playing with costs and seeing what you get based on your system
        - Other measures of cost
            - sequence identity
            - biochemical identity (ex. scoring likelihood of change between dif. amino acids - like in Stone Lab)
            - some gaps are more costly than others (ex. at the beginning of the sequence)

    Needleman-Wunsch algorithm
        - dynamic programming algorithm = break up large task into smaller tasks, aka recursive algorithm
            - evaluating cost of aligning two sites assumes that all sequence before those sites has been aligned already
        - requires 2 sequences and indel/sub costs
        - F = matrix representing the minimum cost to align sequences, each point in matrix reps the cost to align 
        positions with each other (ex a1 with b2 or a1 with b1)
            - F(i,j) -> i = row, j = column
                - add a gap in first position of matrix (upper left corner) then write out sequences and fill in costs
                - build out matrix from upper left corner down, for each box consider the possible answers and 
                determine the cost for each one, then pick the one with the lowest cost
                    - for each box, add the score from the box up+left from it if a substitution, from the box next to
                    it if gaps
                        - See lecture notes from CSL --> Needleman-Wunsch algorithm: Costs
                        - Because: assuming already aligned, so pull value from existing cost of alignment
                    - if two alignments have the same cost, just pick one
        - most alignment methods use Needleman-Wunsch as a base algorithm
        - when evaluating final alignment you always start at the bottom right corner, not by looking for lowest cost
                    

04 February 2025
    What we learned about Needleman-Wunsch is great for two sequences, but what about for many?
        (exapanding from pairwise to multiple sequence alignment)

    Multiple sequence alignment
        1. Progressive alignment:
            - requires a guide tree then builds the tree out from there (leaves to root)
                - guide tree is made using distances, we'll learn this in Lecture 8
                - guide tree requirement is a limitation of this method: poor guide? poor alignment
                    - fast to make, but not accurate
            - takes two sequences at a time and gradually adds more sequences (see figure in Alignment II)
            - pairwidse distances = cost calculated by Needleman-Wunsch
            - How to align alignments
                - computational algorithm, no biology at this stage
                1. convert each alignment into a profile
                    - profile = how frequently we see each nucleotide at each position, ignoring gaps
                2. combine the profiles: cost of alignment based on Needleman-Wunsch (outcome = cost matrix of substitutions)
                    - multiply the probabilities of each nucleotide in each position, pull probabilities from profile
                    - sum the products of the probability multiplying, this is the cost of aligning two positions
                    - repeat for every position and possibility (shout out to computers)
                3. Align profiles using stated cost of gaps and cost matrix for cost of substitution (make F(i,j) table)

    If you want to use a software, read the paper about it! Will give good insight for how to use and strenghts/weaknesses

    Learn@Home: Sequencing --> look for a paper with a phylogeny and follow their data availability section to access data

06 February 2025
    For sequencing homework:
        - add links to paper or databases
        - start documenting the steps for getting the data in my notebook-log.md
        - try to use 10-15 taxa/species: larger ones will take longer to run, especially later in the class
        - troubleshooting datasets: pay attention to file formats (google how to convert files if needed, office hours otherwise)
        - troubleshooting software installation: google errors, huge part of bioninformatics
            - use Slack channel!

13 February 2025
    Clarifications from last class:
    To Do: consolidate sequences into one file with new/descriptive names
        Ex. >Minc-proteinname

        - Alignment scores are only comparable within the same software
            - So... cannot compare score from T-Coffee to Muscle, but can compare scores within Muscle if parameters are different
            between alignment events
        - QualiMap: visualizing quality of alignments

---

    - phylogenetic tree is a mathmatical simplification attempting to represent the history of speciation events
        - external nodes (leaves of tree): extant species
        - internal nodes: common ancestors
        - assumptions: 
            - speciation is instantaneous, represented by strict bifurcation of tree (we know it wasn't)
            - length of tree is linearly correlated with mutation timing (not true)
            - rate of mutation is constant (also not true lol)
            - all trees are binary (each node can only split into 2)
        - from these given sequences, what is the tree that bests represents the evolution of these sequences?
            this is what a phylogenetic tree is!
    - building trees: three methods
        - distance trees: fast, but lose sequence info
        - parsimony scores
        - likelihood score (maximum or bayesian)
        Regardless of method, steps are the same:
            1. choose distance or parsimony or likelihood
            2. guess the tree
            3. evaluate the score of the tree
            4. propose a new tree
            5. evaluate the score of new tree, better than first? keep it! worse than first? back to first one, drop second
            Repeat until all trees are scored and the optimum tree is found (tree with highest score)
    
        Maximum Likelihood recipe: look at slides lol, basically calculates the likelihood of the tree input to the function
            Same for parsimony
            Good practice: use different methods to get different trees and compare to see which are best

    Phylogenetic inference:
        1. choose criterion
        2. search possible trees for optimum

    Weaknesses of above pipeline:
        - We don't know what the function looks like, so we need to evaluate a ton of trees. There's no way to
        evaluate all of them, and as the number of samples increases the number of possible trees is so frickin high
            ex. 52 samples, # of trees > # atoms in universe
            So... use different strategies to try and find optimum, like starting from different trees
            with large numbers of samples, make sure to check how the algorithm decided to stop!
        - Depends on substitution model which is chosen by the user 
        - Depends on quality of data: number, alignment quality, etc.

    Example Maximum Likelihood Software: RAxML (Stamatakis 2006)
        - CSL critique: developers are only focused on speed, not on accuracy. Testing with so many samples in tree that 
        there is no guarantee of a correct tree; did not test enough trees
            - Make sure to pay attention to the tree they start with: the bigger the space, the better the starting tree needs to be

    How to navigate the tree space:
        - 3 ways to adjust trees (LOOK AT CSL'S SLIDES FOR FIGURES), check algorithms for what kind of steps they take
            1. NNI (nearest neighbor interchange): simplest; focus on one edge of the tree that connects two clades, remove the edge and randomly 
            reconnect the clades then evaluate the new tree
                - keeps most of the tree fixed: what happens in each clade is the same, it's just how the clades are
                related to each other
                    - why NNI is often called the "smallest" step you can take in the tree space
            2. SPR (subtree pruning and reconnecting): cut edge and reconnect (I don't understand how this is different from NNI)
            3. TBR: cut edge and add a new edge in between two random edges
            - want a method that tests all three steps
                - only NNI would be really really slow
                - some algorithms ask the user to determine the frequency of each step
            
    Homework: focus on generating alignment for my sequence data!

18 February 2025
    - Distance-based trees are faster because you don't have to explore all of the tree space
        - algorithm relies on how accurate the distances are

    - Choices: 1. what kind of distance 2. what algorithm to build tree
        - types of distance
            - p-distance: proportion of nt sites that differ between two sequences (#dif/total#)
                - visualized/outputted as a matrix with same # of rows/cols as # of sequences and the distances in each entry
                - simplest distance method
                - tends to be biased: mutations don't accumulate at the same rate in dif orgs/seqs
                    - so.. not often used, use a more complex model "of evolutionary distances"
            - evolutionary distances: accounts for mutation rates, most commonly used to calculate distances

    Today: assuming we already have distances, how do we make a tree? (Cluster analysis)
        Simplest algorithm: UPGMA/WPGMA
            Start with a pairwise distance matrix then...
                1. put together taxa with smallest distance
                2. add the next closest by calculating distance from first two to all others
                    d(AB)k = (d(a,k)+d(b,k))/2
                    when doing this, keep track of what you are joining when you make the new matrix
                3. repeat until all taxa are included
                - complication: if there are two idential score, one has to be selected arbitrarily
            As you increase the length of your sequence, you converge on the wrong tree (because it doesn't account of mutation rates)
                So.. most people do not use distance-basd trees, but still good to know and understand
            Also.. lose sequence information, so people don't like that bc going based on distance matrix
            Ultrametric tree: same distance from root to all tips (implicitly assumes same rate of evolution for each extant taxa)

        Neighbor-joining tree:
            Also starts with a pairwise distance matrix
                1. sum all distances for each node (taxa) aka net divergence (r)
                    - accounts for if one node is super far from all the others because it evolves very quickly relative to the rest of the taxa
                2. create rate-corrected distance matrix
                3. look at website, but basically do the UPGMA steps with the corrected rates/distances (with inclusion of branch lengths)
                Be careful! original distances are d, updated distances are r

        Distance-based methods can be run in/through R

20 February 2025 - Guest Lecture
    What is parsimony?
        - Ockham's razor: the simplest explanation is the most likely one
        - what are the fewest changes needed to get to a tree that explains our data?
        - parsimony does not rely on a model of evolution (different from distance methods)
            - useful when more complex methods won't work or if you don't want to make the assumptions associated with other methods
            - good option when rate of evolution/mutation is slow
                    ex. super conserved things
                - not a good option for things that evolve quickly or faster than others (ie noncoding regions)
            - shown to be statistically inconsistent as you increase the number of samples

    Process of Parsimony:
        1. determine the amount of character change to explain the data with a given tree (tree topology)
        2. try all possible tree topologies, scoring each to find the best

        Includes costs for subs and indels (unweighted if costs are the same, weighted if not)

        Fitch algorithm: does the parsimony scores for us, uses equal costs
            - goes through all the internal nodes and computes the changes needed at each node to get this tree
            - parsimony at the tip is zero because no changes need to be made

    Homework: redoing the tree and calculating the scores

    Method for unequal costs: Sankoff algorithm
        Ex. mutation b/w purine and pyrimidine
        Ex. changes between amino acids based on characteristics (polarity, charge, size, etc)
        Still a little arbitrary, but tries to account for differences in costs

    Newick/Paranthetical Format of a tree: ((W,Y),(X,Z));
        Anything in a set of parentheses is a clade, commas are nodes
             /\
            /  \
           /    \
          /\    /\
         /  \  /  \
        W    Y X   Z

        Represent branch length as: ((W:2,Y:3):4,...)
            Branch leading to W has a length of 2, to Y has length of 3, to clade has length of 4
        Semicolon: represents that the tree is rooted, unrooted if ; is absent

    R Crash Course:
        ?functionname: gives info on function in lower right window
        write.tree(treename) = newick format for tree
        write.tree(treename,filename) = makes a file with that tree

25 February 2025 - Guest Lecture, R tutorial
    - today: following along with distance/parsimony-based tutorials

    in R: >?function (like >?dist.dna): tells you all the arguments needed for that function and describes the options for those arguments

    more notes in distance and parsimony R file!

27 February 2025: Models of Evolution
    - distance and parsimony methods are outdated, but they still teach them because it's where the field started
        - now most people use likelihood and bayesian methods

    Starting the slides now:
    - What is a model? A simplification of real life so we can ask and answer questions
        - Example: predicting the number of bikes that will pass your bus stop each day
            use the Poisson model to calculate and use the average number of bikes per day for lambda
                - lambda value (average number of bikes) determines the mean value and shape of the distribution
                - only one parameter (lambda), so not capturing all variability --> this is the point of models, they simplify things for us
                (look at slides for equation) 
                (fun fact from melette: poisson = fish in french)
        - models will never be perfect, have to balance accuracy of model with usefulness of model
            - more accurate = more complicated and more difficult to use
            - Example: Boston subway map is super simplified and definitely not accurate (stops are not equidistant and subway does not travel
                in a straight line), but it's more useful for taking the subway than the super realistic satellite image map
        - How do we know if a model is useful?
            1. are the assumptions accurate?
            2. what is the quality of the input data?
                - if you use a model and get weird results: it might not be the model, it's probably your data!
    
    - In phylogenetics we want the probability that we will see particular mutations in our sequences
        What is the probability that I observe these sequences if I have this tree? (looking at simple tree with DNA slide)
        - Assumptions: not true, but have to do this for our models to be useful; models exist that get around these but we won't worry about them
            (the slides have transitions that help make these clearer)
            1. the mutation process is the same at every branch of the tree, each branch will use the same probability model/equation
                (this assumption exists for all phylogenetic tools we will use)
                - Implication: focus on mutations between two sequences
            2. all sites evolves independently
                - would be computationally intense to do otherwise
                - Implication: we can focus on mutations between two sites
            3. all sites evolve the same (mutation process is the same regardless of what kind of sequence)
                - Implication: we can use any site to model the mutation process
        - How likely is it that we went from A to A in time t?
            - even though we start and end with the same nucleotide, there still could have been mutations throughout t
            - Poisson equation: in slides; mu=mutation rate t=branch length/time
                - mutation rate = number of mutation events
                    - mutation =/= substitution, A>A is still a mutation but not a substitution, A>G is both a substitution and a mutation
                    - so mutation rate/events includes A>C, A>T, A>G AND A>A
                - why a Poisson distribution? no reason, arbitrary decision but now everyone uses it
                - need to know what the probability of each mutation event is, so we need a probability matrix
                    - estimate these numbers from data
                    R probability = probability of going from nt>nt in one step
                    P probability = probability of going from nt>nt in whole branch length/time t

                    i zoned out for a few min

                - choosing a substitution model means choosing Q, which is the likelihood of various base changes, similar to choosing costs
                - Markov chain: jumping between possibilities, you keep track of where it goes
                    - probability of the next step only depends on the current one, ignoring all the steps before the current one (frog jumps)
                - did not go over ergodic or time-reversible assumptions
            - First model ever: Jukes-Cantor (1969) - assumes everything is equally likely and all sites are equally present in data (25% A, 25% G, etc)
                - has explicit formulas for transitions, one of the only ones (most models don't have explicit formulas)

            - Slide with assumptions and papers under them: papers are models that try to fix that assumption
                - new norm for assumption 3: allowing each site to have its own mutation rate
